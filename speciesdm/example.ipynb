{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biogeodataframe import BioGeoDataFrame\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "from rioxarray.merge import merge_arrays\n",
    "from geocube.api.core import make_geocube\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CRS to BC Albers\n",
    "CRS = 'EPSG:3005'\n",
    "GEOCUBE_RES = 500\n",
    "N_SAMPLES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in species occurrence data as a geodataframe and remove non-georeferenced rows\n",
    "species_tmp = gpd.read_file('../data/black_bear_occurrences.csv')\n",
    "species_tmp = species_tmp[(species_tmp['decimalLatitude'] != '') & (species_tmp['decimalLongitude'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the geopandas to a BioGeoDataFrame, giving access to useful methods\n",
    "N = np.nanmin((N_SAMPLES, species_tmp.shape[0]))\n",
    "species_tmp = species_tmp.sample(N)\n",
    "\n",
    "species = BioGeoDataFrame(species_tmp)\n",
    "species = species.set_geometry(gpd.points_from_xy(\n",
    "        species['decimalLongitude'], species['decimalLatitude'])).set_crs(4326)\n",
    "species = species.to_crs(CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in biogeoclimatic zones and reproject to desired CRS\n",
    "# Use only the ZONE and geometry fields, the former of which is what we will predict species' distributions with\n",
    "bec_tmp = gpd.read_file('../data/bec').to_crs(CRS)\n",
    "bec_tmp = bec_tmp[['ZONE', 'SUBZONE', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables must be made numeric to be transformed into a raster, so must convert numbers back to strings\n",
    "# To do this, create list of all strings\n",
    "bec_zones = bec_tmp.ZONE.drop_duplicates().values.tolist()\n",
    "bec_subzones = bec_tmp.SUBZONE.drop_duplicates().values.tolist()\n",
    "categorical_enums = {\"ZONE\": bec_zones, \"SUBZONE\": bec_subzones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bec geodataframe to rioxarray raster\n",
    "# Resolution is in the units of target CRS\n",
    "bec = make_geocube(vector_data = bec_tmp, resolution=(GEOCUBE_RES, -GEOCUBE_RES), categorical_enums=categorical_enums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(bec['ZONE']))\n",
    "# print(np.unique(bec['ZONE'].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric back to categorical string\n",
    "######################################### DO NOT DELETE ######################################### \n",
    "# zone_string = bec['ZONE_categories'][bec['ZONE'].astype(int)].drop('ZONE_categories')\n",
    "# bec['ZONE'] = zone_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pseudo-absences\n",
    "pres_abs = species.add_pseudo_absences(amount=species.shape[0], region_poly=bec_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_DISTANCE = bec.rio.resolution()[1] * 31.5 # in units of CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of raster tiles, find which ones intersect the species occurrence points and are therefore required\n",
    "# Using a single raster, bec, for simplicity\n",
    "rasters = pres_abs.list_rasters(BUFFER_DISTANCE, [bec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of raster tiles into memory\n",
    "# Would load the rasters here, but bec is already loaded for simplicity. Something like:\n",
    "# rasters = [rioxarray.open_rasterio(x) for x in raster]\n",
    "# merged_raster = merge_arrays(rasters)\n",
    "merged_raster = bec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer each point so it intersects adjacent raster cells\n",
    "# pres_abs['buffered_geometry'] = pres_abs['geometry'].buffer(BUFFER_DISTANCE, cap_style=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each occurrence point, build a 3D tensor\n",
    "vals = pres_abs.extract_values(raster=merged_raster, distance=BUFFER_DISTANCE)\n",
    "vals = np.concatenate(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listt = []\n",
    "# listt.append(np.array((((1, 1), (2, 2)), ((1, 1), (2, 2)))))\n",
    "# arr2 = np.array((((2, 2), (4, 4)), ((4, 4), (4, 4))))\n",
    "\n",
    "# # np.stack((listt, arr2)).shape\n",
    "# np.stack((listt))\n",
    "# # arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [x['presence'] for x in vals if None not in x['arr'][0] and 'nodata' not in x['arr'][1]].__len__()\n",
    "# l = [\n",
    "#     x[\"arr\"]\n",
    "#     for x in vals\n",
    "# ]\n",
    "# vals\n",
    "# # [x.shape for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array((((2, 2), (3, 3)), ((2, 2), (3, 3)), ((2, 2), (3, 3))))\n",
    "# np.zeros((255,255,3))\n",
    "# x_train[0].transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "imshow([x[\"arr\"].transpose() for x in vals][10][:,:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.stack(\n",
    "    [x[\"arr\"].transpose() for x in vals]\n",
    ")  # if None not in x['arr'] is not None and 'nodata' not in x['arr']], axis=0)\n",
    "y_data = np.stack(\n",
    "    [x[\"presence\"] for x in vals]\n",
    ")  # if x['arr'] is not None and 'nodata' not in x['arr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = (\n",
    "    x_data[0 : math.ceil(x_data.shape[0] * 0.8)],\n",
    "    x_data[math.ceil(x_data.shape[0] * 0.8) : -1],\n",
    ")\n",
    "\n",
    "y_train, y_test = (\n",
    "    y_data[0 : math.ceil(x_data.shape[0] * 0.8)],\n",
    "    y_data[math.ceil(x_data.shape[0] * 0.8) : -1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#   # tf.keras.layers.Input(shape=(1,)),\n",
    "#   tf.keras.layers.Flatten(),\n",
    "#   tf.keras.layers.Dense(4, activation='relu'),\n",
    "#   tf.keras.layers.Dense(4, activation='relu'),\n",
    "#   tf.keras.layers.Dense(2, activation='softmax')\n",
    "# ])\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(Conv2D(32, (2, 2), input_shape=(64, 64, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # downsample each dimension by a factor of 2\n",
    "\n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2)) # This should be the number of layers\n",
    "model.add(Activation('softmax'))\n",
    "# len(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 3s 20ms/step - loss: 12.0036 - accuracy: 0.5223 - val_loss: 1.7203 - val_accuracy: 0.5412\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 6.5286 - accuracy: 0.5366 - val_loss: 0.8576 - val_accuracy: 0.6145\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 4.4206 - accuracy: 0.5374 - val_loss: 1.2786 - val_accuracy: 0.5592\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 3.1100 - accuracy: 0.5326 - val_loss: 1.7102 - val_accuracy: 0.5271\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 2.2368 - accuracy: 0.5356 - val_loss: 1.4456 - val_accuracy: 0.5241\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 1.7150 - accuracy: 0.5466 - val_loss: 1.7297 - val_accuracy: 0.5201\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 1.3110 - accuracy: 0.5602 - val_loss: 1.0364 - val_accuracy: 0.5472\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 1.1096 - accuracy: 0.5599 - val_loss: 1.3222 - val_accuracy: 0.5231\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.9650 - accuracy: 0.5687 - val_loss: 1.0371 - val_accuracy: 0.5351\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.8722 - accuracy: 0.5745 - val_loss: 1.0058 - val_accuracy: 0.5371\n"
     ]
    }
   ],
   "source": [
    "m = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
